import openai
import requests
import re

# é…ç½®
LLM_API_URL = "http://127.0.0.1:8001/v1"
# æ³¨æ„ï¼šç¡®ä¿ä½ çš„ MCP Server æ­£åœ¨ 8000 ç«¯å£è¿è¡Œ
MCP_SERVER_URL = "http://127.0.0.1:8000" 

client = openai.OpenAI(api_key="empty", base_url=LLM_API_URL)

def call_mcp_tool(tool_name, from_station, to_station):
    url = f"{MCP_SERVER_URL}/tools/{tool_name}"
    params = {"from_name": from_station, "to_name": to_station}
    try:
        response = requests.get(url, params=params)
        if response.status_code == 200:
            # ğŸ’¡ æ³¨æ„è¿™é‡Œï¼šå»æ‰ .get('result')ï¼Œç›´æ¥è·å– json
            data = response.json() 
            
            # æ‰“å°ä¸€ä¸‹ï¼Œæ–¹ä¾¿ä½ åœ¨é»‘çª—å£è°ƒè¯•çœ‹æ•°æ®å¯¹ä¸å¯¹
            print(f"DEBUG Server Response: {data}")
            
            steps = data.get('distance_steps', '-')
            time = data.get('time_minutes', '-')
            path_info = data.get('path', 'æ— å…·ä½“è·¯å¾„')
            return f"ğŸ“ è·¯å¾„è§„åˆ’ï¼š\n{path_info}\n\nğŸ“Š ç»Ÿè®¡æ•°æ®ï¼šå…±ç»è¿‡ {steps} ç«™ï¼Œé¢„è®¡è€—æ—¶ {time} åˆ†é’Ÿã€‚"
        else:
        # æå–æœåŠ¡å™¨è¿”å›çš„ç²¾ç¡® detail
            try:
                err_detail = response.json().get('detail', 'è®¡ç®—å‡ºé”™')
                return f"âŒ {err_detail},ç«™åè¾“å…¥æ¨¡ç³Šï¼Œè¯·è¾“å…¥æ­£ç¡®ç«™åï¼ˆå¦‚ï¼šå…‰ç†™é—¨ç«™ï¼Œè€Œéå…‰ç†™å®¶å›­ï¼‰"
            except:
                return "âŒ åœ°é“æœåŠ¡å™¨å“åº”å¼‚å¸¸ã€‚"
    except Exception as e:
        return f"æ— æ³•è¿æ¥åˆ°åœ°é“æœåŠ¡å™¨: {str(e)}"

def run_ai_agent(user_input):
    # 1. æ„é€ å¼ºçº¦æŸçš„ System Prompt
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªåŒ—äº¬åœ°é“ä¸“å®¶ã€‚ç”¨æˆ·ä¼šå‘ä½ å’¨è¯¢åœ°é“è·¯å¾„ã€‚
    å¦‚æœéœ€è¦æŸ¥è¯¢è·¯å¾„ï¼Œè¯·**å¿…é¡»**æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›å¤ï¼Œä¸è¦æœ‰ä»»ä½•å¤šä½™æ–‡å­—ï¼š
    CALL:tool_name(from="èµ·ç‚¹ç«™", to="ç»ˆç‚¹ç«™")
    å¯é€‰å·¥å…·åï¼šget_distance_efficient_path (æœ€çŸ­è·¯ç¨‹), get_time_efficient_path (æœ€å¿«æ—¶é—´)
    
    ä¾‹å¦‚ï¼šCALL:get_time_efficient_path(from="ç§¯æ°´æ½­", to="è¥¿ç›´é—¨")
    """
    
    # 2. è°ƒç”¨å¾®è°ƒåçš„ Qwen æ¨¡å‹
    response = client.chat.completions.create(
        model="qwen",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_input}
        ],
        temperature=0.1 # é™ä½éšæœºæ€§ï¼Œè®©æ ¼å¼æ›´å›ºå®š
    )
    
    content = response.choices[0].message.content.strip()
    
    # 3. è§£ææ¨¡å‹æ˜¯å¦å‘å‡ºäº†è°ƒç”¨æŒ‡ä»¤
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ï¼šCALL:å·¥å…·å(from="xxx", to="xxx")
    match = re.search(r'CALL:(\w+)\(from="(.*?)", to="(.*?)"\)', content)
    
    if match:
        tool_name, start, end = match.groups()
        print(f"--- æ­£åœ¨è°ƒç”¨å·¥å…·: {tool_name} ---")
        return call_mcp_tool(tool_name, start, end)
    else:
        # å¦‚æœ AI ç›´æ¥å›ç­”äº†ï¼ˆæ²¡æœ‰è§¦å‘å·¥å…·ï¼‰ï¼Œåˆ™ç›´æ¥è¿”å› AI çš„è¯
        return content