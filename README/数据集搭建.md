上传到LLaMA-Factory的数据集形式有如下几条：
**Alpaca格式**：
1.naive Alpaca（普通指令对形式）
	· 单回合：![[carbon (11).png]]
	· 多回合（有上文模拟记忆）：
	![[carbon (12).png]]

2.Alpaca + chain-of-thought-->"rational"（带思考指令对）
![[carbon (13) 1.png]]

3.偏好格式--chosen/rejected
![[carbon (14).png]]

**ShareGPT格式**
1.naive ShareGPT（多角色会话格式）
![[carbon (10).png]]
此样本形式相当于将一个完整的会话保留在了一个样本之中（学习人与GPT的问答，GPT学习调用外部插件，GPT学习返回的value），下面针对**组成元素**进行分析：
“system”和Alpaca中的input很像，都是系统指令级添加的上下文；
“conversation”就是一个消息列表的title，如同C中的struct一样；
“from”或“role”表示角色。常有“human/user”、“gpt/assistant”、“function_call”、“observation”等。其中“observation”一般用来表示工具/函数调用的返回值，即模型发起function_call后，调用外部工具返回的结果，并用value表示出来。
	· 注意，“function_call”调用外部插件（plugin），如，可以调用本地脚本（爬虫）、公司内部API，第三方Web API（天气、地图、翻译等）、检索/知识库系统、模拟器或计算引擎。
	而训练样本中的“observation”是直接接受人手动调用外部插件生成的返回值。
	
“meta”表示每个对话的元信息容器，自由存放关于整个会话或该样本的附加信息
	常见用途：
	
	- 标注信息：`{"annotator":"张三","quality":"high","label_tags":["math","rationale"]}`
	    
	- 来源/版权：`{"source":"webchat-export","origin":"sharegpt","url":"..."}`
	    
	- 语言与地区：`{"lang":"zh","locale":"zh-CN"}`
	    
	- 时间戳/会话ID/版本：`{"created_at":"2026-02-01T08:00:00Z","dataset_version":"v1.2"}`
	    
	- 训练用权重：`{"weight":1.5}`（用于后处理/采样/再训练时加权）



在训练的时候，ShareGPT的样本形式，需要在输入进模型训练前进行预处理，有三种方法：
1.逐对拆分：对于简单的人机对话会话，humen/user+ system（+history）作为input，gpt/assistant作为output。
2.上下文截断保留法：将最近N轮上下文按照对应规则拼接进input和output。
3.对于function_call/observation：